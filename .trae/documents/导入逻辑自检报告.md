# 导入逻辑自检报告

## 概述

对 `/Users/yi/Code/duckkb/src/duckkb/core/mixins/import_.py` 的导入逻辑进行全面审查，发现以下潜在缺陷。

---

## 🔴 严重问题

### 1. 索引更新时旧 chunk 未清理

**位置**: `_upsert_nodes_sync` + `_build_index_for_ids_sync`

**问题描述**:
当节点内容更新时，如果新文本的 chunk 数量少于旧文本，多余的旧 chunk 索引不会被删除。

**示例场景**:
```yaml
# 第一次导入：文本长度 3000 字符，分成 3 个 chunk (seq 0, 1, 2)
- type: Document
  title: "doc1"
  content: "很长的文本..."  # 3000 字符

# 第二次导入：文本缩短到 1000 字符，只有 1 个 chunk (seq 0)
- type: Document
  title: "doc1"
  content: "短文本"  # 1000 字符
```

**结果**: `search_index` 表中仍保留 seq=1 和 seq=2 的旧索引条目，导致搜索可能返回已删除的内容片段。

**建议修复**:
在 `_build_index_for_ids_sync` 之前，先删除这些 ID 的旧索引：

```python
def _build_index_for_ids_sync(self, upserted_ids: dict[str, list[int]]) -> dict[str, int]:
    # 先删除旧索引
    for node_type, ids in upserted_ids.items():
        if not ids:
            continue
        node_def = self.ontology.nodes.get(node_type)
        if node_def is None:
            continue
        table_name = node_def.table
        placeholders = ", ".join(["?" for _ in ids])
        self.conn.execute(
            f"DELETE FROM {SEARCH_INDEX_TABLE} WHERE source_table = ? AND source_id IN ({placeholders})",
            [table_name] + ids,
        )
    # 然后重建索引...
```

---

### 2. 边删除时未清理索引

**位置**: `_delete_edges_sync`

**问题描述**:
删除边时只删除了边表中的记录，但没有清理 `search_index` 表中可能存在的边索引（如果边类型配置了搜索字段）。

**影响**: 如果边类型有搜索配置，删除边后索引表中会残留悬空记录。

**建议修复**:
参考 `_delete_nodes_sync` 的实现，在删除边时同步清理索引。

---

### 3. 并发导入无锁保护

**位置**: `import_knowledge_bundle`

**问题描述**:
整个导入过程没有锁机制，如果多个协程同时调用 `import_knowledge_bundle`，可能导致：
- 事务冲突
- 影子目录竞态条件
- 数据不一致

**建议修复**:
添加异步锁保护：

```python
class ImportMixin(BaseEngine):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self._import_lock = asyncio.Lock()
    
    async def import_knowledge_bundle(self, temp_file_path: str) -> dict[str, Any]:
        async with self._import_lock:
            # 原有逻辑...
```

---

## 🟡 中等问题

### 4. 临时文件泄漏

**位置**: `import_knowledge_bundle` 第 123 行

**问题描述**:
临时文件删除在主流程最后执行。如果在此之前的任何步骤失败（如向量计算、影子导出），临时文件不会被清理。

**建议修复**:
使用 `try-finally` 确保临时文件被清理：

```python
try:
    # 导入逻辑...
finally:
    if path.exists():
        try:
            await self._unlink_file(path)
        except Exception:
            logger.warning(f"Failed to cleanup temp file: {path}")
```

---

### 5. 向量计算失败后索引不完整

**位置**: `_compute_vectors_async`

**问题描述**:
向量计算在事务提交后异步执行。如果计算失败，`search_index` 表中对应记录的 `vector` 字段为 NULL，导致向量搜索结果不完整。

**影响**: 用户可能不知道某些内容没有被正确向量化。

**建议修复**:
- 在返回结果中明确报告向量计算失败的记录
- 或者提供重试机制

---

### 6. 时间戳精度导致 backup 目录冲突

**位置**: `_atomic_replace_data_dir` 第 1178 行

**问题描述**:
使用毫秒级时间戳命名 backup 目录，在同一毫秒内多次调用可能产生相同名称，导致 `os.rename` 失败。

**建议修复**:
添加随机后缀或使用更高精度时间戳：

```python
import uuid
timestamp = int(time.time() * 1000000)  # 微秒
backup_dir = data_dir.parent / f"{data_dir.name}_backup_{timestamp}_{uuid.uuid4().hex[:8]}"
```

---

### 7. SQL 拼接潜在风险

**位置**: 多处使用 f-string 构建 SQL

**问题描述**:
虽然表名来自 ontology 定义，但如果 ontology 配置被污染，可能存在 SQL 注入风险。

**涉及代码**:
- 第 177 行: `SELECT 1 FROM {table_name}`
- 第 432 行: `INSERT OR REPLACE INTO {table_name}`
- 第 506-509 行: `DELETE FROM {table_name}`

**建议修复**:
使用 `validate_table_name` 函数验证所有动态表名（已在 `storage.py` 中使用）。

---

## 🟢 轻微问题

### 8. 未使用的 import

**位置**: 第 7 行

**问题描述**:
`import time` 仅在第 1178 行使用一次，可以考虑与 `datetime` 合并或使用 `datetime.now().timestamp()` 替代。

**建议**: 保持现状即可，不影响功能。

---

### 9. 哈希冲突理论风险

**位置**: `compute_deterministic_id` (storage.py)

**问题描述**:
使用 SHA256 前 16 位十六进制（64 bit）作为 ID，理论上存在冲突可能（生日攻击约 2^32 次计算）。

**影响**: 对于知识库场景，记录数通常远小于冲突阈值，风险可接受。

---

### 10. 影子目录清理异常被静默忽略

**位置**: 第 134-140 行

**问题描述**:
异常处理中清理影子目录失败时，异常被静默忽略，可能导致磁盘空间泄漏。

**建议修复**:
至少记录警告日志：

```python
except Exception as cleanup_error:
    logger.warning(f"Failed to cleanup shadow directory: {cleanup_error}")
```

---

## 📋 建议修复优先级

| 优先级 | 问题 | 影响 |
|--------|------|------|
| P0 | 索引旧 chunk 未清理 | 搜索结果错误 |
| P0 | 并发导入无锁保护 | 数据损坏 |
| P1 | 临时文件泄漏 | 磁盘空间泄漏 |
| P1 | 边删除未清理索引 | 搜索结果错误 |
| P2 | 向量计算失败处理 | 搜索不完整 |
| P2 | 时间戳精度问题 | 偶发失败 |
| P3 | SQL 拼接风险 | 安全隐患 |

---

## 🔍 测试建议

1. **索引更新测试**: 验证节点内容更新后，旧 chunk 索引是否被正确清理
2. **并发导入测试**: 模拟多个协程同时导入，验证数据一致性
3. **异常恢复测试**: 模拟各阶段失败，验证资源清理是否完整
4. **边删除测试**: 验证删除边时索引是否被清理

---

## 总结

导入逻辑整体设计合理，遵循了原子同步协议。主要问题集中在：
1. **索引生命周期管理** - 更新/删除时旧索引清理不完整
2. **并发安全** - 缺少锁机制
3. **资源清理** - 异常路径下资源可能泄漏

建议优先修复 P0 级别的问题，确保数据一致性。
