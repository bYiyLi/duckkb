# DuckKB 导入流程数据丢失风险分析

## 一、当前架构概述

### 1.1 数据存储架构

```
┌─────────────────────────────────────────────────────────────┐
│                      DuckDB (内存模式)                        │
│  ┌─────────────┐  ┌─────────────┐  ┌──────────────────────┐ │
│  │ 业务表(节点) │  │ 业务表(边)  │  │ _sys_search_index    │ │
│  └─────────────┘  └─────────────┘  │ _sys_search_cache    │ │
│                                     └──────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
                              ▲
                              │ 导入时写入
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                      文件系统 (真理源)                        │
│  data/                                                       │
│  ├── nodes/{table_name}/**/*.jsonl                          │
│  ├── edges/{edge_name}/**/*.jsonl                           │
│  └── cache/search_cache.parquet                             │
└─────────────────────────────────────────────────────────────┘
```

### 1.2 导入流程

```
import_knowledge_bundle()
    │
    ├─1. 读取 YAML 文件
    ├─2. Schema 校验
    ├─3. 开启事务
    │     ├─ 导入节点 (业务表)
    │     ├─ 导入边 (业务表)
    │     ├─ 验证边引用完整性
    │     ├─ 构建索引 (search_index)
    │     └─ 提交事务 (失败则回滚)
    ├─4. 异步计算向量嵌入 ⚠️ 事务外
    ├─5. 影子导出
    │     ├─ 创建 data_shadow/ 目录
    │     ├─ 导出业务数据 (JSONL)
    │     └─ 导出缓存数据 (PARQUET)
    ├─6. 原子替换 data/ 目录
    └─7. 删除临时文件
```

---

## 二、发现的严重问题

### 🔴 问题 1：引擎初始化不加载数据（严重）

**位置**: [engine.py:99-109](file:///c:/Users/baiyihuan/code/duckkb/src/duckkb/core/engine.py#L99-L109)

```python
def initialize(self) -> Self:
    self.sync_schema()          # 只同步表结构
    self.create_index_tables()  # 创建索引表
    return self                 # ❌ 没有加载已有数据！
```

**影响**:
- 知识库已有 JSONL 数据文件
- 重启服务后，DuckDB 内存数据库为空
- **所有搜索返回空结果，数据"丢失"**

**复现步骤**:
1. 导入知识包，数据持久化到 `data/` 目录
2. 重启 MCP 服务或 CLI 命令
3. 执行搜索 → 返回空结果
4. 数据文件存在，但无法访问

---

### 🔴 问题 2：MCP 服务启动不加载数据（严重）

**位置**: [duck_mcp.py:16-36](file:///c:/Users/baiyihuan/code/duckkb/src/duckkb/mcp/duck_mcp.py#L16-L36)

```python
@lifespan
async def engine_lifespan(server: FastMCP[Any]) -> AsyncIterator[dict[str, Any]]:
    duck_mcp = cast("DuckMCP", server)
    duck_mcp.initialize()  # ❌ 只初始化表结构，不加载数据
    yield {"engine": duck_mcp}
    duck_mcp.close()       # ❌ 关闭时也不保存数据
```

**影响**: 同问题 1，MCP 服务重启后数据不可访问

---

### 🟡 问题 3：向量计算在事务外异步执行（中等）

**位置**: [import_.py:120](file:///c:/Users/baiyihuan/code/duckkb/src/duckkb/core/mixins/import_.py#L120)

```python
# 事务已提交
result = await self._execute_import_in_transaction(nodes_data, edges_data)
# ...
vector_result = await self._compute_vectors_async(upserted_ids)  # ⚠️ 事务外
```

**影响**:
- 事务提交后、向量计算完成前崩溃
- 数据已持久化到 JSONL
- 但 `search_index.vector` 字段为 NULL
- **向量搜索功能失效**

**恢复方案**: 需要手动触发向量重建

---

### 🟡 问题 4：缓存不恢复（中等）

**位置**: [index.py:364-388](file:///c:/Users/baiyihuan/code/duckkb/src/duckkb/core/mixins/index.py#L364-L388)

导入时导出了缓存：
```python
# import_.py:1256-1279
cache_count = await self._dump_cache_to_parquet(shadow_dir)
```

但初始化时没有恢复：
```python
# engine.py:99-109
def initialize(self) -> Self:
    self.sync_schema()
    self.create_index_tables()
    # ❌ 没有调用 load_cache_from_parquet()
    return self
```

**影响**:
- 每次重启都需要重新计算分词和向量
- 浪费计算资源和 API 调用费用
- 向量 API 调用可能产生额外费用

---

### 🟢 问题 5：删除节点时边的处理（已正确处理）

**位置**: [import_.py:460-504](file:///c:/Users/baiyihuan/code/duckkb/src/duckkb/core/mixins/import_.py#L460-L504)

```python
def _delete_nodes_sync(self, node_type: str, items: list[dict[str, Any]]) -> ...:
    # ...
    self._delete_edges_for_nodes(record_ids)  # ✅ 先删除相关边
    self._delete_index_for_ids(table_name, record_ids)  # ✅ 再删除索引
    # 最后删除节点
```

**评估**: 此部分设计正确，在事务内级联删除，无数据丢失风险

---

## 三、问题根因分析

### 3.1 设计假设 vs 实际实现

| 设计意图 | 实际实现 | 问题 |
|---------|---------|------|
| JSONL 是真理源 | ✅ 正确 | - |
| 启动时从真理源加载 | ❌ 未实现 | 数据不可访问 |
| 关闭时保存到真理源 | ❌ 未实现 | 数据丢失 |
| 向量计算原子性 | ❌ 事务外 | 向量可能缺失 |

### 3.2 数据流缺失环节

```
启动流程（当前）:
  Engine.initialize() → sync_schema() → create_index_tables() → 结束
                                              ↓
                                        DuckDB 为空 ❌

启动流程（应有）:
  Engine.initialize() → sync_schema() → create_index_tables()
                                              ↓
                           load_all_nodes() → load_all_edges() → load_cache()
                                              ↓
                                        DuckDB 恢复数据 ✅
```

---

## 四、修复方案

### 方案 A：在 initialize() 中加载数据（推荐）

**修改文件**: `engine.py`

```python
async def initialize(self) -> Self:
    """初始化引擎。"""
    self.sync_schema()
    self.create_index_tables()
    
    # 加载已有数据
    await self._load_existing_data()
    
    return self

async def _load_existing_data(self) -> None:
    """从文件系统加载已有数据。"""
    data_dir = self.config.storage.data_dir
    
    if not data_dir.exists():
        return
    
    # 加载所有节点类型
    for node_type in self.ontology.nodes.keys():
        try:
            await self.load_node(node_type)
        except Exception as e:
            logger.warning(f"Failed to load node type {node_type}: {e}")
    
    # 加载所有边类型
    for edge_name in self.ontology.edges.keys():
        try:
            await self.load_edge(edge_name)
        except Exception as e:
            logger.warning(f"Failed to load edge type {edge_name}: {e}")
    
    # 加载缓存
    cache_path = data_dir / "cache" / "search_cache.parquet"
    if cache_path.exists():
        await self.load_cache_from_parquet(cache_path)
```

### 方案 B：修改 MCP lifespan 为异步

**修改文件**: `duck_mcp.py`

```python
@lifespan
async def engine_lifespan(server: FastMCP[Any]) -> AsyncIterator[dict[str, Any]]:
    duck_mcp = cast("DuckMCP", server)
    logger.info("Initializing knowledge base engine...")
    
    # 同步初始化
    duck_mcp.sync_schema()
    duck_mcp.create_index_tables()
    
    # 异步加载数据
    await duck_mcp._load_existing_data()
    
    logger.info("Knowledge base engine initialized.")
    try:
        yield {"engine": duck_mcp}
    finally:
        # 可选：关闭前保存数据
        await duck_mcp._save_data()
        duck_mcp.close()
```

### 方案 C：向量计算纳入事务（可选增强）

将向量计算移到事务内，或使用两阶段提交：

```python
async def import_knowledge_bundle(self, temp_file_path: str) -> dict[str, Any]:
    # ... 前面的步骤 ...
    
    # 方案 1: 在事务内计算向量（可能阻塞较久）
    result = await self._execute_import_in_transaction(
        nodes_data, edges_data, compute_vectors=True
    )
    
    # 或方案 2: 使用补偿事务
    try:
        vector_result = await self._compute_vectors_async(upserted_ids)
    except Exception as e:
        # 标记需要重建向量的记录
        await self._mark_vectors_pending(upserted_ids)
        logger.warning(f"Vector computation failed, marked for retry: {e}")
```

---

## 五、建议的修复优先级

| 优先级 | 问题 | 修复方案 | 影响 |
|-------|------|---------|------|
| P0 | 引擎初始化不加载数据 | 方案 A 或 B | 服务重启后数据完全不可访问 |
| P1 | 缓存不恢复 | 方案 A 的一部分 | 重复计算、API 费用浪费 |
| P2 | 向量计算事务外 | 方案 C | 向量搜索可能失效 |

---

## 六、总结

**当前导入流程存在严重的数据丢失风险**：

1. **最严重**：服务重启后，已有数据无法加载到内存数据库，导致数据"丢失"
2. **中等风险**：向量计算失败时，向量搜索功能失效
3. **性能问题**：缓存不恢复导致重复计算

**建议立即修复问题 1**，否则系统无法在生产环境使用。
