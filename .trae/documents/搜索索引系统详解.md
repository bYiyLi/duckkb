# DuckKB 搜索索引系统详解

## 为什么需要三个表？

用一个简单的类比来理解：

```
想象你在图书馆工作：

1. _sys_search_index  →  图书索引卡片（记录每本书在哪个书架、哪个位置）
2. _sys_search_cache  →  速记本（记录已经翻译/分类过的内容，避免重复劳动）
3. _sys_search_index_fts →  关键词检索目录（专门用于快速查找关键词）
```

---

## 核心问题：为什么要这样设计？

### 问题 1：为什么不直接在原表上搜索？

**假设场景**：你有一个 `articles` 表，存储了 1000 篇文章，每篇文章平均 5000 字。

```
❌ 直接搜索的问题：
- 文章太长，搜索效率低
- 无法做语义搜索（找"相似含义"的内容）
- 无法追踪搜索结果来自文章的哪个段落

✅ 使用索引表的方案：
- 把长文章切成小片段（chunk）
- 每个片段单独索引
- 搜索时精确定位到具体段落
```

### 问题 2：为什么需要缓存表？

**关键洞察**：分词和向量化是**昂贵的计算**。

```
场景：100 篇文章都引用了同一句名言"知识就是力量"

❌ 没有缓存：
- 对这 100 个片段分别调用分词 API → 100 次
- 对这 100 个片段分别调用向量化 API → 100 次（还要花钱！）

✅ 有缓存：
- 第 1 次计算：分词 + 向量化 → 存入 cache（用 content_hash 作为 key）
- 后续 99 次：直接从 cache 读取 → 0 次额外计算
```

**content_hash 的作用**：
```python
# 相同内容 = 相同哈希
hash("知识就是力量") = "a1b2c3d4..."
hash("知识就是力量") = "a1b2c3d4..."  # 完全一样！

# 所以可以用 hash 作为缓存的唯一标识
```

### 问题 3：为什么 FTS 要单独一个视图？

**DuckDB FTS 扩展的要求**：FTS 索引需要特定的表结构。

```sql
-- FTS 扩展要求：
-- 1. 必须有一个 doc_id 列（唯一标识）
-- 2. 必须指定要索引的文本列

-- 所以创建视图来满足这个要求：
CREATE VIEW _sys_search_index_fts AS
SELECT 
    source_table || '_' || source_id || '_' || source_field || '_' || chunk_seq as doc_id,
    content
FROM _sys_search_index
```

---

## 完整数据流图解

### 索引构建流程

```
┌─────────────────────────────────────────────────────────────────────┐
│                        原始数据                                      │
│  articles 表                                                         │
│  ┌─────┬────────────────────────────────────────┐                  │
│  │ id  │ content                                 │                  │
│  ├─────┼────────────────────────────────────────┤                  │
│  │  1  │ "这是一篇很长的文章，内容涉及人工智能..."  │                  │
│  │  2  │ "知识就是力量，学习使人进步..."          │                  │
│  └─────┴────────────────────────────────────────┘                  │
└─────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     Step 1: 文本切片                                 │
│  ChunkingMixin.chunk_text()                                         │
│                                                                      │
│  "这是一篇很长的文章..." → ["这是一篇很长的", "文章，内容涉及", ...]    │
│                                                                      │
│  每个切片约 800 字，有 100 字重叠，确保不丢失上下文                     │
└─────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     Step 2: 计算哈希                                 │
│                                                                      │
│  content_hash = MD5("这是一篇很长的")                                │
│                                                                      │
│  用于：                                                              │
│  1. 检查缓存是否已存在                                               │
│  2. 作为缓存表的主键                                                  │
└─────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     Step 3: 分词（可选）                             │
│  TokenizerMixin.segment()                                           │
│                                                                      │
│  "这是一篇很长的" → "这是 一篇 很 长 的"                             │
│                                                                      │
│  用于全文搜索（FTS），让搜索更精准                                    │
│                                                                      │
│  ⚡ 先查缓存：SELECT fts_content FROM cache WHERE hash = ?          │
│  ⚡ 命中则跳过计算，未命中则调用 jieba 分词                           │
└─────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     Step 4: 向量化（可选）                           │
│  EmbeddingMixin.embed_single()                                      │
│                                                                      │
│  "这是一篇很长的" → [0.123, -0.456, 0.789, ...] (1536维向量)         │
│                                                                      │
│  用于语义搜索，找"含义相似"的内容                                     │
│                                                                      │
│  ⚡ 先查缓存：SELECT vector FROM cache WHERE hash = ?               │
│  ⚡ 命中则跳过计算，未命中则调用 OpenAI API（要花钱！）               │
└─────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     Step 5: 写入索引表                               │
│                                                                      │
│  _sys_search_index:                                                 │
│  ┌─────────────┬───────────┬───────────────┬───────────┬─────────┐ │
│  │source_table │ source_id │ source_field  │ chunk_seq │ content │ │
│  ├─────────────┼───────────┼───────────────┼───────────┼─────────┤ │
│  │ articles    │    1      │ content       │    0      │ 这是... │ │
│  │ articles    │    1      │ content       │    1      │ 文章... │ │
│  │ articles    │    2      │ content       │    0      │ 知识... │ │
│  └─────────────┴───────────┴───────────────┴───────────┴─────────┘ │
│                                                                      │
│  _sys_search_cache:                                                 │
│  ┌─────────────┬─────────────┬──────────────────────┐              │
│  │content_hash │ fts_content │ vector               │              │
│  ├─────────────┼─────────────┼──────────────────────┤              │
│  │ a1b2c3...   │ 这是 一篇... │ [0.123, -0.456, ...] │              │
│  └─────────────┴─────────────┴──────────────────────┘              │
└─────────────────────────────────────────────────────────────────────┘
```

### 搜索查询流程

```
┌─────────────────────────────────────────────────────────────────────┐
│                     用户输入查询                                     │
│                                                                      │
│  query = "人工智能应用"                                              │
└─────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     Step 1: 向量化查询                               │
│                                                                      │
│  "人工智能应用" → [0.234, -0.567, 0.890, ...]                       │
└─────────────────────────────────────────────────────────────────────┘
                              │
              ┌───────────────┴───────────────┐
              ▼                               ▼
┌───────────────────────────┐   ┌───────────────────────────┐
│     向量搜索               │   │     全文搜索               │
│                           │   │                           │
│  计算余弦相似度            │   │  使用 BM25 算法            │
│                           │   │                           │
│  SELECT ...               │   │  SELECT ...               │
│  WHERE array_cosine_sim   │   │  WHERE match_bm25(        │
│    (vector, query_vec)    │   │    content, '人工智能')    │
│  > 0.7                    │   │                           │
│                           │   │                           │
│  结果：语义相似的内容      │   │  结果：关键词匹配的内容    │
└───────────────┬───────────┘   └───────────────┬───────────┘
                │                               │
                └───────────────┬───────────────┘
                                  ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     Step 2: RRF 融合排序                             │
│                                                                      │
│  RRF (Reciprocal Rank Fusion) 公式：                                 │
│                                                                      │
│  score = α/(k + rank_vector) + (1-α)/(k + rank_fts)                │
│                                                                      │
│  其中：                                                              │
│  - α = 向量搜索权重（默认 0.7）                                      │
│  - k = RRF 常数（默认 60）                                           │
│  - rank = 各自结果中的排名                                           │
│                                                                      │
│  效果：同时考虑语义相似度和关键词匹配度                               │
└─────────────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────────────┐
│                     Step 3: 返回结果                                 │
│                                                                      │
│  [                                                                   │
│    {                                                                 │
│      source_table: "articles",                                       │
│      source_id: 1,                                                   │
│      source_field: "content",                                        │
│      chunk_seq: 3,                                                   │
│      content: "人工智能在医疗领域的应用...",                          │
│      score: 0.85                                                     │
│    },                                                                │
│    ...                                                               │
│  ]                                                                   │
└─────────────────────────────────────────────────────────────────────┘
```

---

## 三个表的关系总结

```
                    ┌─────────────────────────────────────┐
                    │         _sys_search_index          │
                    │                                     │
                    │  主索引表，存储所有可搜索内容        │
                    │  - 原始内容 (content)               │
                    │  - 分词结果 (fts_content)           │
                    │  - 向量嵌入 (vector)                │
                    │  - 来源追溯 (source_table/id/field) │
                    │                                     │
                    └──────────────┬──────────────────────┘
                                   │
                    ┌──────────────┼──────────────┐
                    │              │              │
                    ▼              ▼              ▼
        ┌───────────────┐  ┌───────────────┐  ┌───────────────┐
        │    cache      │  │    FTS View   │  │   搜索查询    │
        │               │  │               │  │               │
        │ 避免重复计算   │  │ 供 BM25 使用  │  │ 向量+全文混合 │
        │ - 分词缓存    │  │               │  │               │
        │ - 向量缓存    │  │               │  │               │
        └───────────────┘  └───────────────┘  └───────────────┘
              ▲
              │
        通过 content_hash
        关联相同内容
```

---

## 关键设计决策

### 1. 为什么用 content_hash 而不是直接用 content 做缓存 key？

```
✅ 用 hash：
- 固定长度（32 字符），索引高效
- 内容相同 → hash 相同，自动去重
- 不暴露原始内容（隐私）

❌ 用 content：
- 长度不固定，可能很长
- 索引效率低
- 需要额外的去重逻辑
```

### 2. 为什么切片（chunk）而不是整篇索引？

```
✅ 切片索引：
- 搜索结果更精准（定位到段落）
- 向量表示更准确（短文本语义更集中）
- 可以追踪具体位置

❌ 整篇索引：
- 搜索结果太粗糙
- 长文本的向量表示会"稀释"语义
- 无法定位具体段落
```

### 3. 为什么需要两种搜索方式？

```
向量搜索：擅长语义理解
- 查询："机器学习"
- 能找到："深度学习、神经网络、AI 算法..."（语义相关）

全文搜索：擅长精确匹配
- 查询："机器学习"
- 只找到：包含"机器学习"这个词的内容

混合搜索 = 两者结合 = 最佳效果
```
